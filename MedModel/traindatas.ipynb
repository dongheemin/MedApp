{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import input\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "if tf.gfile.Exists(FLAGS.checkpoint_dir) == False:\n",
    "    tf.gfile.MakeDirs(FLAGS.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "images, labels = input.get_data('train', FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    var = tf.Variable(initial, name=name)\n",
    "    return var\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    var = tf.Variable(initial, name=name)\n",
    "    return var\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x, height, width):\n",
    "    return tf.nn.max_pool(x, ksize=[1, height, width, 1], strides=[1, height, width, 1], padding='SAME')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32 64\n",
      "Tensor(\"MaxPool:0\", shape=(10, 32, 32, 32), dtype=float32)\n",
      "16 16 128\n",
      "Tensor(\"MaxPool_1:0\", shape=(10, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(10, 16, 16, 64), dtype=float32) 16384\n"
     ]
    }
   ],
   "source": [
    "num_conv = FLAGS.num_conv\n",
    "kernel_size = FLAGS.kernel_size\n",
    "pool_size = FLAGS.pool_size\n",
    "num_map = FLAGS.num_map\n",
    "num_fc_layer = FLAGS.num_fc_layer\n",
    "num_fc_input = FLAGS.num_fc_input\n",
    "\n",
    "height = FLAGS.height\n",
    "width = FLAGS.width\n",
    "prev_num_map = FLAGS.depth\n",
    "h_pool = images\n",
    "\n",
    "for i in range(num_conv):\n",
    "    W_conv = weight_variable([kernel_size, kernel_size, prev_num_map, num_map], 'W_conv' + str(i+1)) \n",
    "    #1.19x19x3x32\n",
    "    #2.19x19x32x64\n",
    "    b_conv = bias_variable([num_map], 'b_conv' + str(i+1))\n",
    "    h_conv = tf.nn.relu(conv2d(h_pool, W_conv) + b_conv)\n",
    "    h_pool = max_pool(h_conv, pool_size, pool_size)\n",
    "    #1. \n",
    "    prev_num_map = num_map\n",
    "    num_map = int(round(num_map*2))\n",
    "    height = int(round(height/2))\n",
    "    width = int(round(width/2))\n",
    "    print(height,width,num_map)\n",
    "    print(h_pool)\n",
    "\n",
    "num_map = (int)(num_map/2)\n",
    "print(h_pool, height*width*num_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fc_input = tf.reshape(h_pool, [-1, height * width * num_map])\n",
    "prev_num_fc_input = height * width * num_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_fc_layer):\n",
    "    W_fc = weight_variable([prev_num_fc_input, num_fc_input], 'W_fc' + str(i+1))\n",
    "    b_fc = bias_variable([num_fc_input], 'b_fc' + str(i+1))\n",
    "    h_fc = tf.nn.relu(tf.matmul(h_fc_input, W_fc) + b_fc)\n",
    "    h_fc_input = tf.nn.dropout(h_fc, keep_prob)\n",
    "    prev_num_fc_input = num_fc_input\n",
    "    num_fc_input = (int)(num_fc_input/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-7920c9a27232>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-7920c9a27232>\"\u001b[1;36m, line \u001b[1;32m29\u001b[0m\n\u001b[1;33m    conv1_vals, cutoff1_vals = sess.run([h_conv, MaxPool_1:0], feed_dict={keep_prob: 1.0})\u001b[0m\n\u001b[1;37m                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "num_fc_input = (int)(num_fc_input*2)\n",
    "W_fc = weight_variable([num_fc_input, FLAGS.num_class], 'W_fc' + str(i+2))\n",
    "b_fc = bias_variable([FLAGS.num_class], 'b_fc' + str(i+2))\n",
    "\n",
    "hypothesis = tf.matmul(h_fc_input, W_fc) + b_fc\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=hypothesis, labels=labels))\n",
    "train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)\n",
    "\n",
    "cost_sum = tf.summary.scalar(\"cost\", cross_entropy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    if tf.gfile.Exists(FLAGS.checkpoint_dir + '/test'):\n",
    "        saver.restore(sess, FLAGS.checkpoint_dir + '/test')\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"./logs/cost_log_test\",sess.graph)\n",
    "    merge_sum = tf.summary.merge_all()\n",
    "\n",
    "    for step in range(FLAGS.max_steps):\n",
    "        summary, _ = sess.run([merge_sum,train_step], feed_dict={keep_prob: 0.7})\n",
    "        writer.add_summary(summary, step)\n",
    "        conv1_vals, cutoff1_vals = sess.run([h_conv, h_pool], feed_dict={keep_prob: 1.0})\n",
    "        fig = plt.figure(figsize=(16,16))\n",
    "        for f in range():\n",
    "            subplot = fig.add_subplot(16, 16, f+1)\n",
    "            subplot.set_xticks([])\n",
    "            subplot.set_yticks([])\n",
    "            subplot.imshow(conv1_vals[0,:,:,f], cmap=plt.cm.gray_r, interpolation='nearest') \n",
    "            plt.show()\n",
    "        print(step, sess.run(cross_entropy, feed_dict={keep_prob: 1.0}))\n",
    "        if step % 100 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "            saver.save(sess, FLAGS.checkpoint_dir + '/test')\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
